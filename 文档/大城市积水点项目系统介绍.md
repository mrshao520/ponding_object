# 大城市暴雨积水点多源数据实时采集处理系统介绍

## 第一章 项目背景

全球气候变化和快速城市化打破了城市降水—汇水—排水原有的平衡, 加剧了中国城市洪涝问题, 造成了巨大的生命和财产损失。因此, 亟须探索精确、高效的对洪涝灾害事件进行监测分析方法, 提高城市防洪抗灾能力, 降低灾害损失影响。

已经有相当多的研究对洪涝灾害事件进行监测分析，诸如分析受灾范围与受灾人口，集成遥感与深度学习的方法提取积水深度等。然而传统的传感器网络提供的数据时空分辨率不足，且卫星对地表观测容易受到云层的遮挡，致使访问周期变长。与此同时，城市洪涝渍水的特点是影响时间较短，这也对数据的时间分辨率（密度）产生较高的需求。

以多源数据为代表的社会感知方法是物理感知网络的一种重要补充，主要是指时空标记的大数据以及基于这种大数据的方法和应用，着重于对社会经济领域，在自然灾害的监测与预警中同样发挥着重要的作用，是一种快速、低成本、高效的监测方式。社会感知数据相较于传感网络有着数据海量，覆盖面积广，观测密度高且可以详细的记录人类行为模式的优点。然而社会感知方法获取的海量的观测数据有着严重的噪声问题。社会感知的数据来源多样，结构不一致且有效信息密度很低。以社交媒体数据为代表的社会感知数据，大量有价值的信息淹没在大量无关消息之中。对自然灾害的描述信息，多以文本描述形式存在。这些不规则的信息无疑为科学研究带来了一定的影响，凭借人工进行筛选费时费力且不具有时效性。

所以我们的系统利用网络爬虫等技术实现针对微博、抖音等渠道的大城市暴雨积水点多源数据实时获取，使用预训练的UIE模型对获取的数据进行处理，提取如积水点，时间，积水深度等有效信息，形成北京市等大城市近5年积水点数据集。

## 第二章 项目内容

利用网络爬虫等技术实现针对微博、抖音等渠道的大城市暴雨积水点多源数据实时获取，使用预训练的UIE模型对获取的数据进行处理，提取如积水点、时间和积水深度等有效信息，形成北京市等大城市近5年积水点数据集。

主要有以下内容：

1. 完成从抖音、微博等渠道实现大城市暴雨积水点多源数据获取；

2. 完成对多源数据获取系统实时部署；

3. 完成对获取数据的去重、辩伪、时域对齐等数据清洗；

4. 完成对数据清洗系统实时部署；

5. 形成大城市历史积水点数据集；

6. 涉及的大城市数量不低于10个；

7. 历史积水点时间序列数据集不低于5年；

8. 实现业务运行日志、监控与报警。



## 第三章 开发环境及相关技术介绍

### 3.1 开发环境

本系统使用的开发环境为Microsoft Windows 11专业版，版本为22621.2715。使用百度Paddle Paddle深度学习框架训练UIE模型，使用通过爬虫收集整得到的城市历史积水点信息作为数据集。本系统使用C/S架构，使用的开发工具为Visual Studio Code和各种插件。前端使用PyQt5和ECharts可视化显示，使用的模块化管理工具Anaconda；后端使用Python Web框架Flask进行Web后端代码编写，使用FlastDeploy进行模型的快速化部署。



### 3.2 相关技术介绍

#### 3.2.1 PaddlePaddle

Paddle 是一个开源的深度学习框架，由百度推出。它包含了各种深度学习模型和工具，可以帮助开发者更快速、高效地构建和训练深度学习模型。Paddle 支持多种深度学习模型，包括卷积神经网络 (CNN)、循环神经网络 (RNN)、生成对抗网络 (GAN)、自动编码器 (AE) 等。它还提供了多种预训练模型，可以直接使用，满足各种不同应用场景的需求。

Paddle 具有高度优化的计算图和高性能的 C++ 后端，可以有效加速模型的训练和推理。它提供了多种分布式训练方式，可以支持跨机器、跨 GPU 的训练，满足大规模数据的需求。Paddle 支持动态图和静态图两种编程模型，可以灵活地应对不同的场景和需求。

Paddle 提供了丰富的文档和指导，可以帮助开发者快速上手，并开展深度学习项目。它还有一个活跃的社区，可以为开发者提供帮助和支持。Paddle 的目标是成为最简单、最高效的深度学习框架

#### 3.2.2 UIE

UIE(Universal Information Extraction)：Yaojie Lu等人在ACL-2022中提出了通用信息抽取统一框架UIE。该框架实现了实体抽取、关系抽取、事件抽取、情感分析等任务的统一建模，并使得不同任务间具备良好的迁移和泛化能力。为了方便大家使用UIE的强大能力，PaddleNLP借鉴该论文的方法，基于ERNIE 3.0知识增强预训练模型，训练并开源了首个中文通用信息抽取模型UIE。该模型可以支持不限定行业领域和抽取目标的关键信息抽取，实现零样本快速冷启动，并具备优秀的小样本微调能力，快速适配特定的抽取目标。

![img](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/167236006-66ed845d-21b8-4647-908b-e1c6e7613eb1.png)

UIE的优势：

- **使用简单**：用户可以使用自然语言自定义抽取目标，无需训练即可统一抽取输入文本中的对应信息。**实现开箱即用，并满足各类信息抽取需求**。
- **降本增效**：以往的信息抽取技术需要大量标注数据才能保证信息抽取的效果，为了提高开发过程中的开发效率，减少不必要的重复工作时间，开放域信息抽取可以实现零样本（zero-shot）或者少样本（few-shot）抽取，**大幅度降低标注数据依赖，在降低成本的同时，还提升了效果**。
- **效果领先**：开放域信息抽取在多种场景，多种任务上，均有不俗的表现。

#### 3.2.3 FastDeploy

FastDeploy是一款全场景、易用灵活、极致高效的AI推理部署工具， 支持云边端部署。提供超过 160+ Text，Vision， Speech和跨模态模型开箱即用的部署体验，并实现端到端的推理性能优化。包括物体检测、字符识别（OCR）、人脸、人像扣图、多目标跟踪系统、NLP、Stable Diffusion文图生成、TTS 等几十种任务场景，满足开发者多场景、多硬件、多平台的产业部署需求。FastDeploy的目标是为开发者提供一个简单、高效、可靠的AI模型部署工具，以加速AI技术在各个产业的应用落地。该工具的开源性使得全球的开发者都可以参与到其开发和完善中来，共同推动AI技术的进步和应用。

![img](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/213087733-7f2ea97b-baa4-4b0d-9b71-202ff6032a30.png)



#### 3.2.4 Flask

Flask是一个Python Web开发框架。Flask轻巧、简洁，性能上只能满足平常的Web的开发需求，其他功能的完成都需要引用额外的扩展，因此Flask不是最有名，但是Flask是最简单的框架。



#### 3.2.5 scrapy

Scrapy 是一个适用于 Python 的一个快速、高层次的、开放源代码的屏幕抓取和 web 抓取的应用程序框架，用于抓取 web 站点并从页面中提取结构化的数据，可用于各种有用的应用程序。Scrapy 常应用在包括数据挖掘、监测、信息处理、存储历史数据（历史档案）或自动化测试等一系列的程序中。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。Scrapy 最初是为屏幕抓取而设计的，但它也可以用来访问 API 或用作通用 Web 搜寻器来提取数据。

Scrapy的优点：

* 提高效率: Scrapy使用Twisted异步网络库，允许多个请求同时进行，从而提高爬取效率。异步请求使得在等待某些请求的同时，可以继续发送其他请求，最大程度地利用网络带宽和系统资源。
* 降低延迟:异步请求能够减少请求和响应之间的等待时间，加速数据获取过程。这对于大规模的爬取任务尤为重要。
* 数据结构化:选择器帮助将爬取的信息以结构化的形式存储，便于后续处理和分析。



## 功能介绍

### 客户端模块功能介绍

#### 模块结构图

![image-20240115105712539](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115105712539.png)

![image-20240115110324937](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115110324937.png)



#### 显示模块

中心地图可视化显示区域，将显示通过爬虫收集整理得到的每个城市5年的历史积水点总数。其中，总数值最大的五个城市将以呼吸灯效果进行显示。同时，在获得积水深度值的前提下，若积水深度值超过一定的阈值，将采用红色呼吸灯的效果显示在中心地图可视化区域。

城市历年积水点柱状图显示区域，将显示每个城市最近5年的积水点个数，该区域有三种显示方式，分别为柱状图显示，折线图显示，以及总数柱状图显示，默认为柱状图显示。

城市积水点汇总饼状图显示区域，将显示每个城市发生的历史积水点总数。

![image-20240115150219628](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115150219628.png)

![image-20240115111148486](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115111148486.png)

![image-20240115111206518](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115111206518.png)

![image-20240115111237570](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115111237570.png)



#### 属性设置模块

属性设置模块中，共有城市选取模块，采集渠道设置模块和采集属性设置模块。城市选取模块中可以选取超大型城市和大型城市，并且可以通过配置文件增加或删除城市。采集渠道设置模块中可以选取搜狐、微博、腾讯和抖音等渠道。采集属性设置模块中，共有定时采集模块，实时数据采集模块和历史数据采集模块。实时数据采集模块默认当天的数据为实时数据，开始时间为当日凌晨，结束时间为当前时间。历史数据采集模块中，可以手动设置开始时间和结束时间。定时采集模块中，可以设置系统自动爬取平台数据的间隔时间，比如每30分钟采集一次。

![image-20240115145920779](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115145920779.png)



#### 功能模块

功能模块中，共有采集模块，模型处理模块和历史数据模块。采集模块中，选取想要采集的属性后，点击开始采集，即可进行采集。也可以点击停止采集提前终止采集。采集完成后，采集到的数据将会显示在系统的相应区域。点击开始处理，即可将采集到的数据上传至服务端，在服务端将调用所训练的信息处理模型进行关键字段的提取，如日期，城市，地点等信息。并将提取后的信息保存至数据库，在保存过程中将会对数据进行去重、清洗等操作。提取后的信息将传回客户端，客户端将在系统的右下角区域显示处理后的结果，点击保存即可更新显示模块，并将信息保存在本地。历史数据模块中，选取城市后，手动设置开始时间和结束时间，即可进行查询，查询后的结果将在系统中的相应区域显示。选中查询后的结果，点击删除即可删除该数据。

![image-20240115150106313](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115150106313.png)

![image-20240115150115130](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115150115130.png)

![image-20240115150133547](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115150133547.png)

#### 更新功能

修改配置文件之后，如修改城市名或者服务器地址，可以通过系统页面顶部右击鼠标，点击刷新。即可更新页面和配置。每一次刷新都会与服务端进行同步，保证本地的历史数据是最新的。

![1](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/1-17060828271963.png)



### 服务端功能介绍

#### UIE模型训练

##### 训练数据集定制

我们使用数据标注平台doccano 进行数据标注，本示例也打通了从标注到训练的通道，即doccano导出数据后可通过doccano.py脚本轻松将数据转换为输入模型时需要的形式，实现无缝衔接。

标注步骤如下：

* 在doccano平台上，创建一个类型为序列标注的标注项目。

* 定义实体标签类别，上例中需要定义的实体标签有出发地、目的地、费用和时间。

* 使用以上定义的标签开始标注数据，下面展示了一个doccano标注示例：

![image-20240115153130862](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115153130862.png)

* 标注完成后，在doccano平台上导出文件，并将其重命名为doccano_ext.json后，放入./data目录下。

* 执行以下脚本进行数据转换，执行后会在./data目录下生成训练/验证/测试集文件。

```
python doccano.py \
    --doccano_file ./data/doccano_ext.json \
    --task_type ext \
    --save_dir ./data \
    --splits 0.8 0.2 0 \
    --schema_lang ch
```

可配置参数说明：

* doccano_file: 从doccano导出的数据标注文件。

* save_dir: 训练数据的保存目录，默认存储在data目录下。

* negative_ratio: 最大负例比例，该参数只对抽取类型任务有效，适当构造负例可提升模型效果。负例数量和实际的标签数量有关，最大负例数量 = negative_ratio * 正例数量。该参数只对训练集有效，默认为5。为了保证评估指标的准确性，验证集和测试集默认构造全负例。

* splits: 划分数据集时训练集、验证集所占的比例。默认为[0.8, 0.1, 0.1]表示按照8:1:1的比例将数据划分为训练集、验证集和测试集。

* task_type: 选择任务类型，可选有抽取和分类两种类型的任务。

* options: 指定分类任务的类别标签，该参数只对分类类型任务有效。默认为["正向", "负向"]。

* prompt_prefix: 声明分类任务的prompt前缀信息，该参数只对分类类型任务有效。默认为"情感倾向"。

* is_shuffle: 是否对数据集进行随机打散，默认为True。

* seed: 随机种子，默认为1000.

* separator: 实体类别/评价维度与分类标签的分隔符，该参数只对实体/评价维度级分类任务有效。默认为"##"。

* schema_lang: 选择schema的语言，可选有ch和en。默认为ch，英文数据集请选择en。

备注：

* 默认情况下 doccano.py 脚本会按照比例将数据划分为 train/dev/test 数据集

* 每次执行 doccano.py 脚本，将会覆盖已有的同名数据文件

* 在模型训练阶段我们推荐构造一些负例以提升模型效果，在数据转换阶段我们内置了这一功能。可通过negative_ratio控制自动构造的负样本比例；负样本数量 = negative_ratio * 正样本数量。

* 对于从doccano导出的文件，默认文件中的每条数据都是经过人工正确标注的。

##### 模型微调

推荐使用 Trainer API 对模型进行微调。只需输入模型、数据集等就可以使用 Trainer API 高效快速地进行预训练、微调和模型压缩等任务，可以一键启动多卡训练、混合精度训练、梯度累积、断点重启、日志显示等功能，Trainer API 还针对训练过程的通用训练配置做了封装，比如：优化器、学习率调度等。

使用下面的命令，使用 uie-base 作为预训练模型进行模型微调，将微调后的模型保存至./checkpoint/model_best：

单卡启动：

```
python finetune.py  \
    --device gpu \
    --logging_steps 10 \
    --save_steps 100 \
    --eval_steps 100 \
    --seed 42 \
    --model_name_or_path uie-base \
    --output_dir ./checkpoint/model_best \
    --train_path data/train.txt \
    --dev_path data/dev.txt  \
    --max_seq_length 512  \
    --per_device_eval_batch_size 16 \
    --per_device_train_batch_size  16 \
    --num_train_epochs 20 \
    --learning_rate 1e-5 \
    --label_names "start_positions" "end_positions" \
    --do_train \
    --do_eval \
    --do_export \
    --export_model_dir ./checkpoint/model_best \
    --overwrite_output_dir \
    --disable_tqdm True \
    --metric_for_best_model eval_f1 \
    --load_best_model_at_end  True \
    --save_total_limit 1
```

可配置参数说明：

* model_name_or_path：必须，进行 few shot 训练使用的预训练模型。可选择的有 "uie-base"、 "uie-medium", "uie-mini", "uie-micro", "uie-nano", "uie-m-base", "uie-m-large"。

* multilingual：是否是跨语言模型，用 "uie-m-base", "uie-m-large" 等模型进微调得到的模型也是多语言模型，需要设置为 True；默认为 False。

* output_dir：必须，模型训练或压缩后保存的模型目录；默认为 None 。

* device: 训练设备，可选择 'cpu'、'gpu' 、'npu'其中的一种；默认为 GPU 训练。

* per_device_train_batch_size：训练集训练过程批处理大小，请结合显存情况进行调整，若出现显存不足，请适当调低这一参数；默认为 32。

* per_device_eval_batch_size：开发集评测过程批处理大小，请结合显存情况进行调整，若出现显存不足，请适当调低这一参数；默认为 32。

* learning_rate：训练最大学习率，UIE 推荐设置为 1e-5；默认值为3e-5。

* num_train_epochs: 训练轮次，使用早停法时可以选择 100；默认为10。

* logging_steps: 训练过程中日志打印的间隔 steps 数，默认100。

* save_steps: 训练过程中保存模型 checkpoint 的间隔 steps 数，默认100。

* seed：全局随机种子，默认为 42。

* weight_decay：除了所有 bias 和 LayerNorm 权重之外，应用于所有层的权重衰减数值。可选；默认为 0.0；

* do_train:是否进行微调训练，设置该参数表示进行微调训练，默认不设置。

* do_eval:是否进行评估，设置该参数表示进行评估。

##### 模型评估

通过运行以下命令进行模型评估：

```
python evaluate.py \
    --model_path ./checkpoint/model_best \
    --test_path ./data/dev.txt \
    --batch_size 16 \
    --max_seq_len 512
```

可配置参数说明：

* model_path: 进行评估的模型文件夹路径，路径下需包含模型权重文件model_state.pdparams及配置文件model_config.json。

* test_path: 进行评估的测试集文件。

* batch_size: 批处理大小，请结合机器情况进行调整，默认为16。

* max_seq_len: 文本最大切分长度，输入超过最大长度时会对输入文本进行自动切分，默认为512。

* debug: 是否开启debug模式对每个正例类别分别进行评估，该模式仅用于模型调试，默认关闭。

* multilingual: 是否是跨语言模型，默认关闭。

* schema_lang: 选择schema的语言，可选有ch和en。默认为ch，英文数据集请选择en。



##### 定制模型一键预测

paddlenlp.Taskflow装载定制模型，通过task_path指定模型权重文件的路径，路径下需要包含训练好的模型权重文件model_state.pdparams

```
>>> from pprint import pprint
>>> from paddlenlp import Taskflow

>>> schema = ['出发地', '目的地', '费用', '时间']
# 设定抽取目标和定制化模型权重路径
>>> my_ie = Taskflow("information_extraction", schema=schema, task_path='./checkpoint/model_best')
>>> pprint(my_ie("城市内交通费7月5日金额114广州至佛山"))
[{'出发地': [{'end': 17,
           'probability': 0.9975287467835301,
           'start': 15,
           'text': '广州'}],
  '时间': [{'end': 10,
          'probability': 0.9999476678061399,
          'start': 6,
          'text': '7月5日'}],
  '目的地': [{'end': 20,
           'probability': 0.9998511131226735,
           'start': 18,
           'text': '佛山'}],
  '费用': [{'end': 15,
          'probability': 0.9994474579292856,
          'start': 12,
          'text': '114'}]}]
```



#### UIE模型部署

服务端使用Flask Web框架以及FastDeploy AI推理部署工具进行快速化部署UIE模型，并使用Docker容器快速部署环境。使用Flask提供API接口，使用FastDeploy进行模型推理，并使用MySQL进行数据持久化。



## 结构介绍

### 客户端结构介绍

```
project # 项目主目录
|
├── README.md                       # 系统介绍
├── pongind.exe						# 程序
├── requirements.txt				# 系统环境文件
│
└───documents						# 配置文件夹
│   ├── city_volume.json			# 15个城市历史数据文件
│   ├── config.json					# 配置文件
│   └── weibo.json					# 微博账号信息文件
│   
└───log								# 系统日志文件夹
|   └── waterlogging.log			# 系统日志
│   
└───resources						# 资源文件夹
	├── font						# 系统字体文件夹
	|   └─── DS-DIGIT.ttf			# 字体
	|
	├── html						# echarts图表html文件夹
	|   ├── echarts					# echarts文件
	|	├── city_volume_pie.html	# 城市积水点汇总饼状图显示
	|	├── city_year_bar.html		# 城市历年积水点柱状图显示
	|	└── map.html				# 中心地图可视化显示
	|
	├── images						# 图片资源文件夹
    └── chromedriver.exe			# 谷歌浏览器内核文件

```





## 系统总体功能

### 功能流程图

![泳道图](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/%E6%B3%B3%E9%81%93%E5%9B%BE-17060809399601.png)



### 总体功能

本系统采用C/S架构，在属性设置模块中选取想要检索的属性，如城市选取大型城市重庆，采集渠道设置选取腾讯，选取历史数据采集，开始时间为2023-01-15，结束时间为2024-01-15。点击开始采集，即可进行采集。也可以点击停止采集提前终止采集。通过selenium模块模拟用户去访问各平台获取该系统所需的数据对应的url，因为selenium不能实现异步请求所以将所有的url传给scrapy作为起始url。在scrapy中对url进行访问，在下载中间键（DownloaderMiddleware）里更改自己的配置，如添加cookie信息绕过平台的反爬虫。 对scrapy获取的数据进行筛选，如该条数据里包含的时间是否在我所需的范围内以及数据里的文本是否有我所需的信息。最后通过管道(pipelines)对数据进行传输。

 采集完成后，采集到的数据将会显示在系统的相应区域，如下图所示。点击开始处理，即可将采集到的数据上传至服务端，在服务端将调用所训练的信息处理模型进行关键字段的提取，如日期，城市，地点等信息。并将提取后的信息保存至数据库，在保存过程中将会对数据进行去重、清洗等操作。提取后的信息将传回客户端，客户端将在系统的右下角区域显示处理后的结果，点击保存即可更新显示模块，并将信息保存在本地。在获得积水深度值的前提下，如果积水深度值超过一定阈值，将在中心地图显示区域中以红色呼吸灯效果显示。历史数据模块中，选取城市后，手动设置开始时间和结束时间，即可进行查询，查询后的结果将在系统中的相应区域显示。选中查询后的结果，点击删除即可删除该数据。

![image-20240115151357011](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/image-20240115151357011.png)





### 爬虫注意事项

1. 对于微博平台需要定期更新身份信息，需要客户在移动端上登录我所提供的微博账号或者提供一个账号

   ```
   账号：13085559937   密码：zxcv123
   ```

2. 在系统中如何更新微博身份信息
   1. 在系统页面顶部右击鼠标，会出现图中所示
   2. 点击登录微博后请稍等片刻在移动端中确认登录，如果移动端没有反应 请稍等片刻后尝试重新登录

![1](assets/%E5%A4%A7%E5%9F%8E%E5%B8%82%E7%A7%AF%E6%B0%B4%E7%82%B9%E9%A1%B9%E7%9B%AE%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/1.png)

3. 有些平台需要识别验证码所以爬虫代码里用到了超级鹰验证码识别平台，该平台要收费请定期充值避免系统无法使用，超级鹰官网：https://www.chaojiying.com/user/login/

   ```
   对应的账号密码： 账号：caoguimeng   密码：cgm200066
   ```

4. 在使用系统时请勿短时间内获取太多次，避免对网站造成不必要的负担导致IP封禁
